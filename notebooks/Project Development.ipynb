{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed04722c",
   "metadata": {},
   "source": "# **Project 1: Data Analysis for Medical Applications**\n\n---\n\n### **Names:**\n**Andrés Martínez Almazán A01621042** and **Diego Arechiga Bonilla A01621045**\n\n### **Course:**\nModeling Learning with Artificial Intelligence\n\n### **Professor:**\n**Dr. Omar Mendoza Montoya**\n\n### **Date:**\nJune 4, 2025\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cc0aad50",
   "metadata": {},
   "source": "# Data Collection"
  },
  {
   "cell_type": "markdown",
   "id": "c25f2099",
   "metadata": {},
   "source": "### Selected Signal: **Audio Amplitude**\n\n#### Experiment Design:\n1. Silence/ambient (background noise)\n2. Normal speaking (conversation)\n3. Whispering\n4. Shouting (loud voice)\n5. Playing music (high volume)\n6. Applause"
  },
  {
   "cell_type": "markdown",
   "id": "f9e702be",
   "metadata": {},
   "source": "### Collection Protocol:\n\nFor each activity:\n- **Duration:** 40-60 seconds per recording.\n- **Repetitions:** 5 repetitions per activity.\n- **Conditions:** Perform in a controlled environment, without external noise.\n- **Labeling:** Each recording must have a clear label indicating the activity performed.\n\n### Data Collection Protocol:\n1. Start recording in PhyPhox.\n2. Perform the activity for the established time.\n3. Stop recording and save the file.\n4. Label the file with the activity name and repetition number.\n5. Repeat the process for each activity."
  },
  {
   "cell_type": "markdown",
   "id": "4381d120",
   "metadata": {},
   "source": "### Hypothesis:\nThe audio amplitude will be different between each activity, thus allowing automatic classification of the type of sound in the environment."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb11ae1",
   "metadata": {},
   "outputs": [],
   "source": "# imports \nimport pickle\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd"
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "35dea6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "actividades = [\n",
    "    ('sonido_amb', 1),\n",
    "    ('conversacion', 2), \n",
    "    ('susurro', 3),\n",
    "    ('grito', 4),\n",
    "    ('music', 5),\n",
    "    ('aplausos', 6)\n",
    "]\n",
    "\n",
    "experiment_data = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c6d05",
   "metadata": {},
   "outputs": [],
   "source": "# load files \n\nfor actividad_nombre, actividad_id in actividades:\n    print(\"Processing activity:\", actividad_nombre)\n    \n    # load each test file \n    for i in range(1,6):\n        nombre_archivo = f\"../data/{actividad_nombre}_test{i}.csv\"\n        \n        try:\n            df = pd.read_csv(nombre_archivo)\n            \n            # extract audio signal (sound pressure level)\n            if \"Sound pressure level (dB)\" in df.columns:\n                signal_data = df[\"Sound pressure level (dB)\"].values\n                signal_2d = signal_data.reshape(-1, 1) # convert to 2D to adapt the audio signal\n                \n                experiment_data.append((actividad_nombre, actividad_id, signal_2d))\n                print(f\"{nombre_archivo}: {len(signal_data)} valid samples\")\n            \n            else:\n                print(f\"{nombre_archivo}: Column 'Sound pressure level (dB)' not found\")\n                \n        except FileNotFoundError:\n            print(f\"{nombre_archivo}: File not found\")\n        except Exception as e:\n            print(f\"{nombre_archivo}: Error - {e}\")\n                \nprint(f\"\\n=== SUMMARY ===\")\nprint(f\"Total files loaded: {len(experiment_data)}\")\nprint(f\"Structure created successfully\")"
  },
  {
   "cell_type": "markdown",
   "id": "b4801686",
   "metadata": {},
   "source": "# Evaluate classification algorithms"
  },
  {
   "cell_type": "markdown",
   "id": "1bb9dcab",
   "metadata": {},
   "source": "1. Process the data so that each observation has at least 30 variables or characteristics."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4371161",
   "metadata": {},
   "outputs": [],
   "source": "# process data\n\nprint (\"\\n=== PROCESSING DATA ===\")\n\nfeatures = []\nfor tr in experiment_data:\n    \n    # for each signal \n    feat = [tr[1]]  # actividad_id\n    rms = 0 # Root Mean Square\n    \n    for s in range(tr[2].shape[1]):\n        sig = tr[2][:, s] # extract the audio signal\n        \n        # filter NaN values\n        sig = sig[~np.isnan(sig)]\n        \n        if len(sig) > 0: # only process if there is valid data\n            feat.append(np.mean(sig))  # mean\n            feat.append(np.std(sig)) # standard deviation\n            feat.append(stats.skew(sig))  # skewness\n            feat.append(stats.kurtosis(sig)) # kurtosis\n            rms += np.sum(sig**2)  # sum of squares\n            feat.append(np.median(sig))       # median\n            feat.append(np.min(sig))          # minimum\n            feat.append(np.max(sig))          # maximum\n            feat.append(np.ptp(sig))          # Range (peak-to-peak)\n            feat.append(np.var(sig))          # variance\n            feat.append(np.percentile(sig, 10))   # Q10\n            feat.append(np.percentile(sig, 25))   # Q25\n            feat.append(np.percentile(sig, 75))   # Q75\n            feat.append(np.percentile(sig, 90))   # Q90\n            feat.append(np.percentile(sig, 75) - np.percentile(sig, 25))  # IQR\n            feat.append(np.sum(sig**2))       # Total energy\n            feat.append(np.mean(sig**2))      # Average power\n            feat.append(np.sum(np.abs(sig)))  # Sum of absolute values\n            \n            if len(sig) > 1: # calculate velocity and acceleration\n                velocity = np.diff(sig)       # first derivative\n                feat.append(np.mean(velocity))    # Average velocity\n                feat.append(np.std(velocity))     # Velocity deviation\n                \n                if len(velocity) > 1:\n                    acceleration = np.diff(velocity)  # second derivative\n                    feat.append(np.mean(acceleration))  # average acceleration\n                    feat.append(np.std(acceleration))   # acceleration deviation\n                else:\n                    feat.extend([0, 0])\n            else:\n                feat.extend([0, 0, 0, 0])\n            \n            # Zero crossing rate (ZCR)\n            # Calculate zero crossing rate\n            mean_level = np.mean(sig)\n            zero_crossings = np.sum(np.diff(np.signbit(sig - mean_level)))\n            feat.append(zero_crossings / len(sig))  # zero crossing rate\n            \n            \n            from scipy import stats as scipy_stats\n            feat.append(scipy_stats.moment(sig, moment=3))  # Third moment\n            feat.append(scipy_stats.moment(sig, moment=4))  # Fourth moment\n            feat.append(np.mean(np.abs(sig - np.median(sig))))  # Median absolute deviation\n            feat.append(len(sig))  # Number of samples\n            \n            # Characteristics\n            feat.append(np.max(sig) / np.std(sig))  # Approximate SNR\n            feat.append(np.percentile(sig, 90) - np.percentile(sig, 10))  # Dynamic range\n            \n            # Spectral centroid (temporal approximation)\n            if np.sum(np.abs(sig)) > 0:\n                weighted_sum = np.sum(np.abs(sig) * np.arange(len(sig)))\n                feat.append(weighted_sum / np.sum(np.abs(sig)))  # Spectral centroid\n            else:\n                feat.append(0)\n            \n            # Spectral flux (average change)\n            if len(sig) > 1:\n                feat.append(np.mean(np.abs(np.diff(sig))))  # Spectral flux\n            else:\n                feat.append(0)\n                \n        else:\n            # If there is no valid data, add zeros\n            feat.extend([0] * 29)  # 29 additional characteristics + the original 4 = 33 total    \n            \n        \n        \n            \n    rms = np.sqrt(rms)\n    feat.append(rms)  # RMS (final characteristic)\n    features.append(feat)\n    \nprint(f\"Features extracted per sample: {len(features[0])-1}\")  # -1 because the first element is the ID\nprint(f\"Total samples processed: {len(features)}\")\n\n# Build x and y arrays \nprocessed_data = np.array(features)\nx = processed_data[:,1:]  # Features\ny = processed_data[:,0]   # Labels\n\nprint(f\"\\nShape of X (features): {x.shape}\")\nprint(f\"Shape of y (labels): {y.shape}\")\nprint(f\"Unique labels: {np.unique(y)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "8bff640a",
   "metadata": {},
   "source": "2. Evaluate the performance of classification models:\n   - **SVM**\n   - **SVM with radial basis**\n   - **LDA**\n   - **K-NN**\n    - **MLP**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac8e9d",
   "metadata": {},
   "outputs": [],
   "source": "# Imports\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict, LeaveOneOut, RepeatedKFold, cross_val_score"
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ba5c6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_path = \"../data/processed/audio_features.txt\" \n",
    "processed_data= np.loadtxt(processed_data_path)\n",
    "\n",
    "X = processed_data[:, 1:]  # Características\n",
    "y = processed_data[:, 0]    # Labels\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler())\n",
    "X_scaled = pipeline.fit_transform(X)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04181e71",
   "metadata": {},
   "source": [
    "Support Vector Machine evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b284f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         5\n",
      "         2.0       0.57      0.80      0.67         5\n",
      "         3.0       1.00      1.00      1.00         5\n",
      "         4.0       0.60      0.60      0.60         5\n",
      "         5.0       0.67      0.40      0.50         5\n",
      "         6.0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.80        30\n",
      "   macro avg       0.81      0.80      0.79        30\n",
      "weighted avg       0.81      0.80      0.79        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "clf = SVC(kernel = 'linear')\n",
    "y_pred = cross_val_predict(clf, X_scaled, y, cv=n_folds)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf91af23",
   "metadata": {},
   "source": [
    "Support Vector Machine with radial base evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e10a3033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         5\n",
      "         2.0       0.57      0.80      0.67         5\n",
      "         3.0       1.00      1.00      1.00         5\n",
      "         4.0       0.60      0.60      0.60         5\n",
      "         5.0       1.00      0.60      0.75         5\n",
      "         6.0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.83        30\n",
      "   macro avg       0.86      0.83      0.84        30\n",
      "weighted avg       0.86      0.83      0.84        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "clf = SVC(kernel = 'rbf', random_state=42)\n",
    "y_pred = cross_val_predict(clf, X_scaled, y, cv=n_folds)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7afaf",
   "metadata": {},
   "source": [
    "LDA evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "51bb4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.80      0.89         5\n",
      "         2.0       0.75      0.60      0.67         5\n",
      "         3.0       0.83      1.00      0.91         5\n",
      "         4.0       0.71      1.00      0.83         5\n",
      "         5.0       0.75      0.60      0.67         5\n",
      "         6.0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.83        30\n",
      "   macro avg       0.84      0.83      0.83        30\n",
      "weighted avg       0.84      0.83      0.83        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "n_folds = 5\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "y_pred = cross_val_predict(clf, X_scaled, y, cv=n_folds)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9053ed",
   "metadata": {},
   "source": [
    "k-Nearest Neighbors evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d5b3c2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         5\n",
      "         2.0       0.56      1.00      0.71         5\n",
      "         3.0       1.00      1.00      1.00         5\n",
      "         4.0       0.67      0.40      0.50         5\n",
      "         5.0       1.00      0.60      0.75         5\n",
      "         6.0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.83        30\n",
      "   macro avg       0.87      0.83      0.83        30\n",
      "weighted avg       0.87      0.83      0.83        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n_folds = 5\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "y_pred = cross_val_predict(clf, X_scaled, y, cv=n_folds)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba5737",
   "metadata": {},
   "source": [
    "Multi-layer perceptron evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "21bfdb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         5\n",
      "         2.0       0.57      0.80      0.67         5\n",
      "         3.0       1.00      1.00      1.00         5\n",
      "         4.0       0.75      0.60      0.67         5\n",
      "         5.0       0.75      0.60      0.67         5\n",
      "         6.0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.83        30\n",
      "   macro avg       0.85      0.83      0.83        30\n",
      "weighted avg       0.85      0.83      0.83        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 5\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100), max_iter=10000, random_state=42)\n",
    "y_pred = cross_val_predict(clf, X_scaled, y, cv=n_folds)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c31ef",
   "metadata": {},
   "source": "3. Evaluation of models not seen in class:\n    - **Gaussian Naive Bayes**\n    - **Gradient Boosting**\n    - **Extra Trees**\n    - **Gaussian Process**\n    - **Nearest Centroid**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e94f3",
   "metadata": {},
   "outputs": [],
   "source": "# imports\nfrom sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import NearestCentroid \nfrom sklearn.gaussian_process import GaussianProcessClassifier"
  },
  {
   "cell_type": "markdown",
   "id": "75a14701",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "15073216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         5\n",
      "         2.0       0.80      0.80      0.80         5\n",
      "         3.0       1.00      1.00      1.00         5\n",
      "         4.0       0.80      0.80      0.80         5\n",
      "         5.0       0.80      0.80      0.80         5\n",
      "         6.0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "clf = GaussianNB()\n",
    "y_pred = cross_val_predict(clf, X_scaled, y, cv=n_folds)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762fe9d5",
   "metadata": {},
   "source": [
    "Gradient Boosting evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f43e57db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         5\n",
      "         2.0       0.67      0.80      0.73         5\n",
      "         3.0       1.00      1.00      1.00         5\n",
      "         4.0       1.00      0.80      0.89         5\n",
      "         5.0       0.80      0.80      0.80         5\n",
      "         6.0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.91      0.90      0.90        30\n",
      "weighted avg       0.91      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "clf = GradientBoostingClassifier(n_estimators=50, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "y_pred = cross_val_predict(clf, X_scaled, y, cv=n_folds)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cd0d52",
   "metadata": {},
   "source": "NOTE: An evaluation was attempted with a higher number of decision trees (100), but it overfitted. The same occurred with a lower number of trees (10) and it underfitted. Therefore, an intermediate number of trees (50) was chosen."
  },
  {
   "cell_type": "markdown",
   "id": "ea545a1c",
   "metadata": {},
   "source": [
    "Extra Trees evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a871a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         5\n",
      "         2.0       0.80      0.80      0.80         5\n",
      "         3.0       1.00      1.00      1.00         5\n",
      "         4.0       1.00      1.00      1.00         5\n",
      "         5.0       0.80      0.80      0.80         5\n",
      "         6.0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "clf = ExtraTreesClassifier(n_estimators=50, max_depth=5, random_state=42)\n",
    "y_pred = cross_val_predict(clf, X_scaled, y, cv=n_folds)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f514c4d",
   "metadata": {},
   "source": [
    "Gaussian Process evaluation:\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "10859bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         5\n",
      "         2.0       0.50      0.60      0.55         5\n",
      "         3.0       1.00      1.00      1.00         5\n",
      "         4.0       0.60      0.60      0.60         5\n",
      "         5.0       0.75      0.60      0.67         5\n",
      "         6.0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.80        30\n",
      "   macro avg       0.81      0.80      0.80        30\n",
      "weighted avg       0.81      0.80      0.80        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "clf = GaussianProcessClassifier(random_state=42)\n",
    "y_pred = cross_val_predict(clf, X_scaled, y, cv=n_folds)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5c749",
   "metadata": {},
   "source": [
    "Nearest Centroid evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e63daa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      1.00      1.00         5\n",
      "         2.0       0.57      0.80      0.67         5\n",
      "         3.0       1.00      1.00      1.00         5\n",
      "         4.0       0.60      0.60      0.60         5\n",
      "         5.0       1.00      0.60      0.75         5\n",
      "         6.0       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.83        30\n",
      "   macro avg       0.86      0.83      0.84        30\n",
      "weighted avg       0.86      0.83      0.84        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "clf = NearestCentroid()\n",
    "y_pred = cross_val_predict(clf, X_scaled, y, cv=n_folds)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a8d9dd",
   "metadata": {},
   "source": "# Classification model optimization"
  },
  {
   "cell_type": "markdown",
   "id": "8fea6248",
   "metadata": {},
   "source": "### 1. Model selection:\n\n    - **SVM with radial basis**\n    - **Gradient Boosting**"
  },
  {
   "cell_type": "markdown",
   "id": "777b2c68",
   "metadata": {},
   "source": "SVM with radial basis hyperparameters:\n- **C:** Is the regularization parameter that controls the margin of separation between classes. A higher value allows a narrower margin, while a lower value allows a wider margin.\n- **kernel:** Is the type of kernel used to transform the data. In this case, the radial (RBF) kernel is used.\n- **gamma:** Is the radial kernel parameter that controls the shape of the decision function. A higher value means the decision function will be more complex, while a lower value means it will be simpler."
  },
  {
   "cell_type": "markdown",
   "id": "12bcf714",
   "metadata": {},
   "source": "Gradient Boosting hyperparameters:\n- **n_estimators:** Is the number of decision trees used in the model. A higher value means the model will be more complex, while a lower value means it will be simpler. In this case, a value of 50 was used.\n- **max_depth:** Is the maximum depth of each decision tree. A higher value means the model will be more complex, while a lower value means it will be simpler. In this case, a value of 3 was used.\n- **learning_rate:** Is the learning rate that controls how much the model adjusts in each iteration. A higher value means the model will adjust faster, while a lower value means it will adjust slower. In this case, a value of 0.1 was used."
  },
  {
   "cell_type": "markdown",
   "id": "bc021169",
   "metadata": {},
   "source": "### 2. Hyperparameter performance of each model."
  },
  {
   "cell_type": "markdown",
   "id": "a5e943a2",
   "metadata": {},
   "source": "SVM with radial basis gamma values performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdf148",
   "metadata": {},
   "outputs": [],
   "source": "from matplotlib import pyplot as plt\n\ngamma_values = np.logspace(-6, 1, 50)  # From 10^-6 to 10^1\naccuracies_rbf = []\nn_folds = 5\n\n# Test each gamma value\nfor i, gamma in enumerate(gamma_values):\n    model_rbf = SVC(kernel='rbf', gamma=gamma, C=1.0, random_state=42)\n    scores = cross_val_score(model_rbf, X_scaled, y, cv=n_folds, scoring='accuracy')\n    accuracy_rbf = scores.mean()\n    accuracies_rbf.append(accuracy_rbf)\n\n# Plot average accuracy as a function of gamma\nplt.figure(figsize=(12, 6))\nplt.plot(gamma_values, accuracies_rbf, marker='o', linestyle='-')\nplt.title('Average accuracy with radial SVM as a function of gamma value')\nplt.xlabel('Gamma value')\nplt.ylabel('Average accuracy')\nplt.xscale('log')  # Logarithmic scale for better visualization\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "1f8e4947",
   "metadata": {},
   "source": "It can be observed that accuracy remains constant around 0.83 in the low range (10^-6 to 10^-2) of gamma, indicating underfitting with very smooth decision boundaries. Accuracy reaches its maximum near gamma = 0.1 with approximately 0.9 accuracy, representing the optimal balance between bias and variance. For gamma values ≥ 1.0, accuracy drops to random classification levels (~0.17 for 6 classes), indicating severe overfitting where the model memorizes individual training points but does not generalize."
  },
  {
   "cell_type": "markdown",
   "id": "db1c10d3",
   "metadata": {},
   "source": "Gradient Boosting gamma values performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b862e",
   "metadata": {},
   "outputs": [],
   "source": "max_depth_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # max_depth values to test\naccuracies_gb = []\nn_folds = 5\n\n# Test each max_depth value\nfor i, max_depth in enumerate(max_depth_values):\n    model_gb = GradientBoostingClassifier(n_estimators=50, max_depth=max_depth, learning_rate=0.1, random_state=42)\n    scores = cross_val_score(model_gb, X_scaled, y, cv=n_folds, scoring='accuracy')\n    accuracy_gb = scores.mean()  # Average accuracy across folds\n\n    # Store the accuracy for each max_depth\n    accuracies_gb.append(accuracy_gb)\n\n# Plot average accuracy as a function of max_depth\nplt.figure(figsize=(12, 6))\nplt.plot(max_depth_values, accuracies_gb, marker='o', linestyle='-')\nplt.title('Average accuracy with Gradient Boosting as a function of max_depth value')\nplt.xlabel('max_depth value')\nplt.ylabel('Average accuracy')\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "1c618c1c",
   "metadata": {},
   "source": "The graph shows the behavior of the Gradient Boosting model performance as a function of the maximum depth of decision trees, where it is observed that the optimal value is at max_depth=2, with an accuracy of around 93%. Being max_depth=3 the value used in the initial model evaluation demonstrates it was a good choice but not the optimal one."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc0589",
   "metadata": {},
   "outputs": [],
   "source": "n_estimators_values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\naccuracies_gb = []\nn_folds = 5\n\n# Test each n_estimators value\nfor i, n_estimators in enumerate(n_estimators_values):\n    model_gb = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=3, learning_rate=0.1, random_state=42)\n    scores = cross_val_score(model_gb, X_scaled, y, cv=n_folds, scoring='accuracy')\n    accuracy_gb = scores.mean()  # Average accuracy across folds\n\n    accuracies_gb.append(accuracy_gb)\n    \n\n# Plot average accuracy as a function of n_estimators\nplt.figure(figsize=(12, 6))\nplt.plot(n_estimators_values, accuracies_gb, marker='o', linestyle='-')\nplt.title('Average accuracy with Gradient Boosting as a function of n_estimators value')\nplt.xlabel('n_estimators value')\nplt.ylabel('Average accuracy')\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "d44d553a",
   "metadata": {},
   "source": "The graph shows that the Gradient Boosting model accuracy gradually improves from n_estimators=10 until reaching an optimal point around n_estimators=50-60, where accuracy stabilizes at 90%, confirming that the value of 50 estimators used in the initial evaluation was a good choice as it falls within the optimal range that avoids both underfitting (too few trees) and potential overfitting (too many trees), and that significantly increasing the number of estimators beyond 60-70 does not provide substantial improvements in model performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4eb63a",
   "metadata": {},
   "outputs": [],
   "source": "learning_rate_values = [0.01, 0.05, 0.1, 0.2, 0.5]\naccuracies_gb = []\nn_folds = 5\n\n# Test each learning_rate value\nfor i, learning_rate in enumerate(learning_rate_values):\n    model_gb = GradientBoostingClassifier(n_estimators=50, max_depth=3, learning_rate=learning_rate, random_state=42)\n    scores = cross_val_score(model_gb, X_scaled, y, cv=n_folds, scoring='accuracy')\n    accuracy_gb = scores.mean()  # Average accuracy across folds\n\n    accuracies_gb.append(accuracy_gb)\n    \n# Plot average accuracy as a function of learning_rate\nplt.figure(figsize=(12, 6))\nplt.plot(learning_rate_values, accuracies_gb, marker='o', linestyle='-')\nplt.title('Average accuracy with Gradient Boosting as a function of learning_rate value')\nplt.xlabel('learning_rate value')\nplt.ylabel('Average accuracy')\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "03cced11",
   "metadata": {},
   "source": "The graph reveals that the optimal learning_rate is in the range of 0.04-0.1, where accuracy reaches 90%, while very low values like 0.02 result in underfitting because the model learns too slowly and fails to converge effectively, and high values like 0.5 can cause overfitting or training instability, validating that the learning_rate=0.1 used in the initial model evaluation was an optimal choice that allows an adequate balance between convergence speed and learning stability."
  },
  {
   "cell_type": "markdown",
   "id": "dfe8d744",
   "metadata": {},
   "source": "### 3. Feature selection"
  },
  {
   "cell_type": "markdown",
   "id": "cfeda780",
   "metadata": {},
   "source": "SVM with radial basis:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c942d5b",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import SVC\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nprint(\"-----Optimal selection of features using SelectKBest with SVM Radial-----\")\n\n# Feature range\nn_features = range(10, 21, 2)  # From 10 to 20, step 2\n\n# Store results\nresults = []\nbest_selector = None  # To save the optimal selector\n\nfor k in n_features:\n    print(f\"Evaluating with k={k} features...\")\n    accuracies = []\n    \n    for train_index, test_index in skf.split(X_scaled, y):\n        # Divide data\n        x_train = X_scaled[train_index, :]\n        x_test = X_scaled[test_index, :]\n        y_train = y[train_index]\n        y_test = y[test_index]\n        \n        # Feature selection\n        selector = SelectKBest(score_func=f_classif, k=k)\n        x_train_selected = selector.fit_transform(x_train, y_train)\n        x_test_selected = selector.transform(x_test)\n        \n        # Classifier\n        clf = SVC(kernel='rbf', gamma=0.1, C=1.0, random_state=42)\n        clf.fit(x_train_selected, y_train)\n        \n        # Predicting\n        y_pred = clf.predict(x_test_selected)\n        accuracy = accuracy_score(y_test, y_pred)\n        accuracies.append(accuracy)\n        \n    # Store the average accuracy for this k\n    avg_accuracy = np.mean(accuracies)\n    results.append((k, avg_accuracy))\n    \n    # Save the best selector during the loop\n    if len(results) == 1 or avg_accuracy > max(result[1] for result in results[:-1]):\n        best_selector = selector\n    \n    print(\"Average accuracy for k=\", k, \":\", avg_accuracy)\n    \n# Optimal number of features (calculate after the loop)\nresults_array = np.array(results)\nopt_index = np.argmax(results_array[:, 1])  # Get index of maximum accuracy\noptimal_k = results_array[opt_index, 0]\noptimal_accuracy = results_array[opt_index, 1]\n\n# Ensure that best_selector reflects the optimal\nif optimal_k != results[opt_index][0]:  # If we need to recreate the selector\n    for train_index, test_index in skf.split(X_scaled, y):\n        x_train = X_scaled[train_index, :]\n        y_train = y[train_index]\n        selector = SelectKBest(score_func=f_classif, k=int(optimal_k))\n        selector.fit(x_train, y_train)\n        best_selector = selector\n        break\n\nprint(\"\\n--- Final Results ---\")\nprint(\"Optimal number of features:\", int(optimal_k))\nprint(\"Optimal accuracy:\", optimal_accuracy)\nprint(\"Selected features (indices):\", np.where(best_selector.get_support())[0])\nprint(\"--------------------------------------------------\")"
  },
  {
   "cell_type": "markdown",
   "id": "558d2e27",
   "metadata": {},
   "source": [
    "Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1635a",
   "metadata": {},
   "outputs": [],
   "source": "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nprint(\"-----Optimal selection of features using SelectKBest with Gradient Boosting-----\")\n\nn_features = range(10, 21, 2) \n\nresults = []\nbest_selector = None\nbest_k = None\n\nfor k in n_features:\n    print(f\"Evaluating with k={k} features...\")\n    accuracies = []\n    \n    for train_index, test_index in skf.split(X_scaled, y):\n        # Divide data\n        x_train = X_scaled[train_index, :]\n        x_test = X_scaled[test_index, :]\n        y_train = y[train_index]\n        y_test = y[test_index]\n        \n        # Feature selection\n        selector = SelectKBest(score_func=f_classif, k=k)\n        x_train_selected = selector.fit_transform(x_train, y_train)\n        x_test_selected = selector.transform(x_test)\n        \n        # Classifier\n        clf = GradientBoostingClassifier(n_estimators=55, max_depth=2, learning_rate=0.1, random_state=42)\n        clf.fit(x_train_selected, y_train)\n        \n        # Predicting\n        y_pred = clf.predict(x_test_selected)\n        accuracy = accuracy_score(y_test, y_pred)\n        accuracies.append(accuracy)\n        \n    # Store the average accuracy for this k\n    avg_accuracy = np.mean(accuracies)\n    results.append((k, avg_accuracy))\n    \n    # Save the best k and selector during the loop\n    if len(results) == 1 or avg_accuracy > max(result[1] for result in results[:-1]):\n        best_k = k\n        best_selector = selector\n    \n    print(\"Average accuracy for k=\", k, \":\", avg_accuracy)\n    \n# Optimal number of features\nresults_array = np.array(results)\nopt_index = np.argmax(results_array[:, 1])  # Get index of maximum accuracy\noptimal_k = results_array[opt_index, 0]\noptimal_accuracy = results_array[opt_index, 1]\n\n# Ensure that best_selector reflects the optimal\nif best_k != optimal_k:\n    for train_index, test_index in skf.split(X_scaled, y):\n        x_train = X_scaled[train_index, :]\n        y_train = y[train_index]\n        selector = SelectKBest(score_func=f_classif, k=optimal_k)\n        selector.fit(x_train, y_train)\n        best_selector = selector\n        break\n\nprint(\"\\n--- Final Results ---\")\nprint(\"Optimal number of features:\", int(optimal_k))\nprint(\"Optimal accuracy:\", optimal_accuracy)\nprint(\"Selected features (indices):\", np.where(best_selector.get_support())[0])\nprint(\"--------------------------------------------------\")"
  },
  {
   "cell_type": "markdown",
   "id": "524792d1",
   "metadata": {},
   "source": "Selected features:\n- **SVM with radial basis:** \n1. Signal mean \n2. Signal median\n3. 25th percentile\n4. 75th percentile\n5. 90th percentile\n6. Average power \n7. Velocity standard deviation (1st derivative)\n8. Third moment\n9. Fourth moment\n10. RMS (final)\n\n- **Gradient Boosting:**\n1. Signal median\n2. Signal variance\n3. 25th percentile\n4. 75th percentile\n5. 90th percentile\n6. Average power\n7. Velocity standard deviation (1st derivative)\n8. Acceleration standard deviation (2nd derivative)\n9. Third moment\n10. Fourth moment"
  },
  {
   "cell_type": "markdown",
   "id": "f7e153ca",
   "metadata": {},
   "source": "### Is it possible to reduce the number of features without losing performance in your model?"
  },
  {
   "cell_type": "markdown",
   "id": "27687fc5",
   "metadata": {},
   "source": "Yes, it is possible to reduce the number of features, in this case, to 10 without losing performance. In the case of SVM, a notable improvement is achieved (from 0.83 to 0.9667). This validates the effectiveness of SelectKBest for optimizing audio amplitude data."
  },
  {
   "cell_type": "markdown",
   "id": "483571cf",
   "metadata": {},
   "source": "### 4. Model evaluation and feature selection with nested cross-validation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae5b5e",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest\n\n# Inner loop for hyperparameter optimization and k\ninner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\n# Outer loop for evaluating the performance of the optimized model\nouter_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) \n\nprint(\"----- SVM with radial basis with Nested Cross-Validation -----\")\n\n# Pipeline: Feature Selection -> SVM Classifier\npipe_svm_nested = Pipeline([('selectkbest', SelectKBest(score_func=f_classif)),('svc', SVC(kernel='rbf', probability=True, random_state=42))])\n\n# Hyperparameter search space (based on previous explorations)\nparam_grid_svm_nested = {'selectkbest__k': [10, 12, 14],  'svc__C': [1.0, 10.0, 20.0],'svc__gamma': [0.05, 0.1, 0.15]}\n\n# Configure GridSearchCV for the inner loop\ngrid_search_svm = GridSearchCV(estimator=pipe_svm_nested, param_grid=param_grid_svm_nested, cv=inner_cv, scoring='accuracy', n_jobs=-1) \n\ny_pred_svm_nested = cross_val_predict(grid_search_svm, X_scaled, y, cv=outer_cv)\n\nprint(\"\\nClassification Report for SVM with radial basis (Nested):\")\nprint(classification_report(y, y_pred_svm_nested))\naccuracy_svm_nested = accuracy_score(y, y_pred_svm_nested)\nprint(f\"General average accuracy (Nested) - SVM with radial basis: {accuracy_svm_nested:.4f}\\n\")\n\n# Train GridSearchCV on all X_scaled data to get global parameters and selected features\ngrid_search_svm.fit(X_scaled, y)\nprint(\"Best hyperparameters:\")\nprint(grid_search_svm.best_params_)\nprint(f\"Best accuracy of internal CV - SVM: {grid_search_svm.best_score_:.4f}\")\n\n# Get and show the features selected by the best global estimator\nbest_pipeline_svm = grid_search_svm.best_estimator_  \nselected_features_mask_svm = best_pipeline_svm.named_steps['selectkbest'].get_support()\nselected_features_indices_svm = np.where(selected_features_mask_svm)[0]\nprint(\"Indices of selected features - SVM:\")\nprint(selected_features_indices_svm)\nprint(\"--------------------------------------------------\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64427afe",
   "metadata": {},
   "outputs": [],
   "source": "print(\"----- Gradient Boosting with Nested Cross-Validation -----\")\n\n# Pipeline: Feature Selection -> Gradient Boosting Classifier\npipe_gb_nested = Pipeline([('selectkbest', SelectKBest(score_func=f_classif)),('gb', GradientBoostingClassifier(random_state=42))])\n\n# Hyperparameter search space (based on previous explorations)\nparam_grid_gb_nested = {'selectkbest__k': [10, 12, 14],'gb__n_estimators': [50, 55, 60], 'gb__max_depth': [2, 3], 'gb__learning_rate': [0.05, 0.1]}\n\n# Configure GridSearchCV for the inner loop\ngrid_search_gb = GridSearchCV(estimator=pipe_gb_nested, param_grid=param_grid_gb_nested, cv=inner_cv, scoring='accuracy',n_jobs=-1)\n\ny_pred_gb_nested = cross_val_predict(grid_search_gb, X_scaled, y, cv=outer_cv)\n\nprint(\"\\nClassification Report for Gradient Boosting (Nested):\")\nprint(classification_report(y, y_pred_gb_nested))\naccuracy_gb_nested = accuracy_score(y, y_pred_gb_nested)\nprint(f\"General average accuracy (Nested) - Gradient Boosting: {accuracy_gb_nested:.4f}\\n\")\n\ngrid_search_gb.fit(X_scaled, y)\nprint(\"Best hyperparameters - Gradient Boosting:\")\nprint(grid_search_gb.best_params_)\nprint(f\"Best accuracy of internal CV - Gradient Boosting: {grid_search_gb.best_score_:.4f}\")\n\nbest_pipeline_gb = grid_search_gb.best_estimator_\nselected_features_mask_gb = best_pipeline_gb.named_steps['selectkbest'].get_support()\nselected_features_indices_gb = np.where(selected_features_mask_gb)[0]\nprint(\"Indices of selected features (by the best global estimator) - Gradient Boosting:\")\nprint(selected_features_indices_gb)\nprint(\"--------------------------------------------------\")"
  },
  {
   "cell_type": "markdown",
   "id": "3ff6e8cd",
   "metadata": {},
   "source": "**SVM with radial basis (Nested):**\n\n*   **General average accuracy (Nested):** 0.9333\n*   **Best hyperparameters (global):**\n    *   `selectkbest__k`: 10\n    *   `svc__C`: 20.0\n    *   `svc__gamma`: 0.15\n*   **Best accuracy of internal CV:** 0.9667\n*   **Indices of selected features:** `[ 4 8 10 11 12 15 18 20 22 23]`\n*   **Selected features:**\n    1.  Index 4: Signal median\n    2.  Index 8: Signal variance\n    3.  Index 10: 25th percentile (Q1)\n    4.  Index 11: 75th percentile (Q3)\n    5.  Index 12: 90th percentile\n    6.  Index 15: Average power\n    7.  Index 18: Velocity standard deviation (1st derivative)\n    8.  Index 20: Acceleration standard deviation (2nd derivative)\n    9.  Index 22: Third moment\n    10. Index 23: Fourth moment\n\n**Gradient Boosting (Nested):**\n\n*   **General average accuracy (Nested):** 0.9333\n*   **Best hyperparameters (global):**\n    *   `gb__learning_rate`: 0.05\n    *   `gb__max_depth`: 2\n    *   `gb__n_estimators`: 50\n    *   `selectkbest__k`: 12\n*   **Best accuracy of internal CV (global):** 0.8667\n*   **Indices of selected features:** `[ 0 4 8 10 11 12 14 15 18 20 22 23]`\n*   **Selected features:**\n    1.  Index 0: Signal mean\n    2.  Index 4: Signal median\n    3.  Index 8: Signal variance\n    4.  Index 10: 25th percentile (Q1)\n    5.  Index 11: 75th percentile (Q3)\n    6.  Index 12: 90th percentile\n    7.  Index 14: Total energy\n    8.  Index 15: Average power\n    9.  Index 18: Velocity standard deviation (1st derivative)\n    10. Index 20: Acceleration standard deviation (2nd derivative)\n    11. Index 22: Third moment\n    12. Index 23: Fourth moment"
  },
  {
   "cell_type": "markdown",
   "id": "658be960",
   "metadata": {},
   "source": "# Online classification"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518323c5",
   "metadata": {},
   "outputs": [],
   "source": "# SINGLE CELL FOR DATA PROCESSING WITH WINDOWS, TRAINING AND SAVING\n# ----- 1. LOADING ORIGINAL DATA (experiment_data) -----\nactividades = [('sonido_amb', 1), ('conversacion', 2), ('susurro', 3),('grito', 4), ('music', 5), ('aplausos', 6)]\nexperiment_data = []\n\nif 'experiment_data' not in locals() or len(experiment_data) == 0:\n    for actividad_nombre, actividad_id in actividades:\n        for i in range(1, 6):\n            nombre_archivo = f\"../data/{actividad_nombre}_test{i}.csv\" \n            try:\n                df = pd.read_csv(nombre_archivo)\n                if \"Sound pressure level (dB)\" in df.columns and \"Time (s)\" in df.columns:\n                    # Get time and signal, remove NaNs from dB signal\n                    time_original = df[\"Time (s)\"].values\n                    signal_original_db = df[\"Sound pressure level (dB)\"].values\n                    \n                    # Remove rows where dB is NaN AND time too (if both are NaN they are useless)\n                    valid_indices_initial = ~np.isnan(signal_original_db)\n                    time_valid = time_original[valid_indices_initial]\n                    signal_valid_db = signal_original_db[valid_indices_initial]\n\n                    if len(time_valid) < 2 or len(signal_valid_db) < 2:\n                        print(f\"File {nombre_archivo} with insufficient data after cleaning NaNs.\")\n                        continue\n                        \n                    experiment_data.append({'actividad_nombre': actividad_nombre, \n                                            'actividad_id': actividad_id, \n                                            'time': time_valid, \n                                            'db_signal': signal_valid_db,\n                                            'nombre_archivo': nombre_archivo})\n                else:\n                    print(f\"Columns 'Time (s)' or 'Sound pressure level (dB)' not found in {nombre_archivo}\")\n            except FileNotFoundError:\n                print(f\"File not found: {nombre_archivo}\")\n            except Exception as e:\n                print(f\"Error loading {nombre_archivo}: {e}\")\n    print(f\"Loaded {len(experiment_data)} files in 'experiment_data'.\")\nelse:\n    print(\"'experiment_data' already exists. Using version in memory.\")\n\n# ----- 2. FUNCTION TO CALCULATE FEATURES \ndef calculate_audio_features_notebook(signal_window):\n    sig = signal_window; sig = sig[~np.isnan(sig)]; features_list = []\n    if len(sig) > 0:\n        features_list.append(np.mean(sig)); features_list.append(np.std(sig))\n        features_list.append(scipy_stats_notebook.skew(sig)); features_list.append(scipy_stats_notebook.kurtosis(sig))\n        features_list.append(np.median(sig)); features_list.append(np.min(sig)); features_list.append(np.max(sig))\n        features_list.append(np.ptp(sig)); features_list.append(np.var(sig))\n        features_list.append(np.percentile(sig, 10)); features_list.append(np.percentile(sig, 25))\n        features_list.append(np.percentile(sig, 75)); features_list.append(np.percentile(sig, 90))\n        features_list.append(np.percentile(sig, 75) - np.percentile(sig, 25)) # IQR\n        features_list.append(np.sum(sig**2)); features_list.append(np.mean(sig**2)); features_list.append(np.sum(np.abs(sig)))\n        if len(sig) > 1:\n            velocity = np.diff(sig); features_list.append(np.mean(velocity)); features_list.append(np.std(velocity))\n            if len(velocity) > 1:\n                acceleration = np.diff(velocity); features_list.append(np.mean(acceleration)); features_list.append(np.std(acceleration))\n            else: features_list.extend([0.0, 0.0]) \n        else: features_list.extend([0.0, 0.0, 0.0, 0.0])\n        mean_level = np.mean(sig)\n        zero_crossings = np.sum(np.diff(np.signbit(sig - mean_level))) if len(sig) > 1 else 0\n        features_list.append(zero_crossings / len(sig) if len(sig) > 0 else 0.0) # ZCR\n        features_list.append(scipy_stats_notebook.moment(sig, moment=3)); features_list.append(scipy_stats_notebook.moment(sig, moment=4))\n        features_list.append(np.mean(np.abs(sig - np.median(sig)))); features_list.append(float(len(sig))) # MAD, Num samples\n        std_sig_val = np.std(sig); features_list.append(np.max(sig) / std_sig_val if std_sig_val > 1e-6 else 0.0) # SNR, avoid div by zero\n        features_list.append(np.percentile(sig, 90) - np.percentile(sig, 10)) # Dynamic range\n        sum_abs_sig = np.sum(np.abs(sig))\n        if sum_abs_sig > 1e-6: # Avoid div by zero\n            weighted_sum = np.sum(np.abs(sig) * np.arange(len(sig))); features_list.append(weighted_sum / sum_abs_sig) # Centroid\n        else: features_list.append(0.0)\n        features_list.append(np.mean(np.abs(np.diff(sig))) if len(sig) > 1 else 0.0) # Flux\n        rms_val = np.sqrt(np.sum(sig**2) / len(sig)) if len(sig) > 0 else 0.0 # RMS\n        features_list.append(rms_val)\n    else: features_list.extend([0.0] * 31) # Fill with floating zeros\n    \n    # Ensure 31 features and that they are floats\n    final_features = []\n    for feat_val in features_list:\n        final_features.append(float(feat_val) if np.isfinite(feat_val) else 0.0)\n\n    if len(final_features) != 31:\n        while len(final_features) < 31: final_features.append(0.0)\n        final_features = final_features[:31]\n    return np.array(final_features)\n\n\n# ----- 3. FEATURE EXTRACTION WITH SLIDING WINDOWS -----\nprint(\"\\nExtracting features with sliding windows...\")\nall_windowed_features = []\n\n# Parameters for windowing and interpolation (same as online script)\ntarget_sampling_rate = 20  # Hz (sampling_rate in online)\nwindow_duration_online = 0.5 # seconds (window_time in online)\nwindow_num_samples_online = int(window_duration_online * target_sampling_rate) # 10 samples\nstep_duration_processing = 0.25 # seconds (50% overlap)\n\nfor entry in experiment_data:\n    actividad_id = entry['actividad_id']\n    original_time = entry['time']\n    original_signal_db = entry['db_signal']\n    file_name_debug = entry['nombre_archivo']\n\n    if len(original_time) < 2 or len(np.unique(original_time)) < 2 : # Cannot interpolate\n        print(f\"  Skipping {file_name_debug}: insufficient time data to interpolate ({len(original_time)} points, {len(np.unique(original_time))} unique).\")\n        continue\n\n    # Interpolate the COMPLETE signal from the file to a high and regular sampling rate (e.g. 100 Hz)\n    # This is to have a consistent base before extracting windows.\n    # Make sure the original time is not problematic\n    if original_time.max() - original_time.min() <= 0:\n        print(f\"  Skipping {file_name_debug}: Original time range is zero or negative.\")\n        continue\n        \n    internal_high_fs = 100 # High internal rate to resample\n    duration_of_recording = original_time[-1] - original_time[0]\n    num_samples_high_fs = int(duration_of_recording * internal_high_fs)\n    if num_samples_high_fs < 2: \n        print(f\"  Skipping {file_name_debug}: Recording duration too short for {internal_high_fs}Hz.\")\n        continue\n\n    t_high_fs = np.linspace(original_time[0], original_time[-1], num_samples_high_fs, endpoint=False)\n    \n    try:\n        from scipy.interpolate import interp1d\n        interp_full_signal = interp1d(original_time, original_signal_db, kind='linear', fill_value=\"extrapolate\", bounds_error=False)\n        signal_high_fs = interp_full_signal(t_high_fs)\n    except ValueError as e:\n        print(f\"  Error interpolating complete signal from {file_name_debug}: {e}\")\n        continue\n\n    # Now apply sliding windows over signal_high_fs\n    samples_per_window_high_fs = int(window_duration_online * internal_high_fs) # e.g. 0.5s * 100Hz = 50\n    samples_per_step_high_fs = int(step_duration_processing * internal_high_fs) # e.g. 0.25s * 100Hz = 25\n    \n    num_windows_created = 0\n    for i in range(0, len(signal_high_fs) - samples_per_window_high_fs + 1, samples_per_step_high_fs):\n        raw_window_high_fs = signal_high_fs[i : i + samples_per_window_high_fs] # High resolution window\n\n        # Interpolate this high resolution window to final resolution (20Hz, 10 samples)\n        # Time for this raw_window_high_fs goes from 0 to window_duration_online\n        t_raw_for_interp = np.linspace(0, window_duration_online, len(raw_window_high_fs), endpoint=False)\n        t_uniform_target = np.linspace(0, window_duration_online, window_num_samples_online, endpoint=False)\n        \n        if len(t_raw_for_interp) < 2 or len(np.unique(t_raw_for_interp)) < 2: continue\n\n        try:\n            interp_func_window = interp1d(t_raw_for_interp, raw_window_high_fs, kind='linear', fill_value=\"extrapolate\", bounds_error=False)\n            interpolated_window_for_features = interp_func_window(t_uniform_target)\n        except ValueError:\n            continue\n            \n        features_1d = calculate_audio_features_notebook(interpolated_window_for_features)\n        all_windowed_features.append(np.concatenate(([actividad_id], features_1d)))\n        num_windows_created +=1\n    # print(f\"  From {file_name_debug}: {num_windows_created} feature windows generated.\")\n\n\nprocessed_data_final = np.array(all_windowed_features)\nif processed_data_final.shape[0] == 0:\n    print(\"ERROR: No features generated. Check windowing logic or input data.\")\n    # Don't continue if there's no data\nelse:\n    print(f\"\\nTotal feature windows generated: {processed_data_final.shape[0]}\")\n    X_original_features = processed_data_final[:, 1:]\n    y_labels = processed_data_final[:, 0]\n\n    print(f\"Shape of X_original_features (windowed): {X_original_features.shape}\")\n    print(f\"Shape of y_labels (windowed): {y_labels.shape}\")\n\n    # ----- 4. FIT AND SAVE StandardScaler -----\n    scaler_to_save = StandardScaler()\n    X_scaled = scaler_to_save.fit_transform(X_original_features)\n    with open('../models/standard_scaler_audio.pkl', 'wb') as f:\n        pickle.dump(scaler_to_save, f)\n    print(\"StandardScaler saved to '../models/standard_scaler_audio.pkl'\")\n\n    # ----- 5. FIT AND SAVE SelectKBest -----\n    k_svm_optimal_global = 10\n    feature_selector_svm_to_save = SelectKBest(score_func=f_classif, k=k_svm_optimal_global)\n    X_scaled_selected_svm = feature_selector_svm_to_save.fit_transform(X_scaled, y_labels)\n    with open('../models/audio_feature_selector_svm.pkl', 'wb') as f:\n        pickle.dump(feature_selector_svm_to_save, f)\n    print(f\"SelectKBest (k={k_svm_optimal_global}) saved to '../models/audio_feature_selector_svm.pkl'\")\n    # print(f\"Selected indices (final): {np.where(feature_selector_svm_to_save.get_support())[0]}\")\n\n\n    # ----- 6. TRAIN AND SAVE FINAL SVM MODEL -----\n    C_svm_optimal_global = 20.0\n    gamma_svm_optimal_global = 0.15\n    final_svm_model_to_save = SVC(kernel='rbf', C=C_svm_optimal_global, gamma=gamma_svm_optimal_global, random_state=42, probability=True)\n    final_svm_model_to_save.fit(X_scaled_selected_svm, y_labels)\n    with open('../models/final_svm_model_svm.pkl', 'wb') as f:\n        pickle.dump(final_svm_model_to_save, f)\n    print(\"Final SVM model saved to '../models/final_svm_model_svm.pkl'\")\n    print(\"\\nProcess completed! .pkl files ready for online application.\")"
  },
  {
   "cell_type": "markdown",
   "id": "c85ee1f4",
   "metadata": {},
   "source": "###  Why the New Processing?\n\nWe redid the data processing and model retraining with new window configurations (short and sliding) for a fundamental reason: to make the features we train the model with identical in nature and scale to those the model will see in real-time during online classification.\n\nPreviously, training features were calculated over complete recordings (long, 40-60 seconds). In contrast, the online application analyzes audio in very short windows (0.5 seconds). A model trained with data from one \"language\" (long window features) will not understand well the data presented in another \"language\" (short window features), resulting in low accuracy.\n\nThe new process ensures that:\n\nAbsolute Consistency: Both training data and online data are now processed by extracting features from 0.5-second windows.\n\nRepresentative Training: The model (StandardScaler, SelectKBest, SVC) learns from thousands of examples of these short windows, capturing the variability and relevant patterns for that specific time span.\n\nHigher Online Accuracy: When the online application calculates features from a new 0.5-second window, these are directly comparable to what the model was trained on, allowing much more precise and reliable predictions."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}